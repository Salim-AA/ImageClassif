{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-13T15:29:25.669987Z",
          "iopub.status.busy": "2021-03-13T15:29:25.669293Z",
          "iopub.status.idle": "2021-03-13T15:29:31.324079Z",
          "shell.execute_reply": "2021-03-13T15:29:31.322763Z"
        },
        "papermill": {
          "duration": 5.667121,
          "end_time": "2021-03-13T15:29:31.324276",
          "exception": false,
          "start_time": "2021-03-13T15:29:25.657155",
          "status": "completed"
        },
        "tags": [],
        "id": "u1EeT9DZPGPd"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf \n",
        "import matplotlib \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np \n",
        "import sklearn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "import seaborn as sn\n",
        "import sys\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q kaggle\n",
        "from google.colab import files\n",
        "\n",
        "files.upload()\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets list #ghir pour tester the kaggle datasets\n",
        "!kaggle datasets download trolukovich/food11-image-dataset/download -p /content/sample_data/ --unzip"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "id": "GoZhA1hwPYXI",
        "outputId": "08cbd903-5d2b-4f7c-8a88-d5300b478009"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-689a1687-e116-461a-a6a2-ae2d1d646d29\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-689a1687-e116-461a-a6a2-ae2d1d646d29\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "ref                                                       title                                               size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
            "--------------------------------------------------------  -------------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n",
            "ruchi798/data-science-job-salaries                        Data Science Job Salaries                            7KB  2022-06-15 08:59:12           6185        208  1.0              \n",
            "surajjha101/bigbasket-entire-product-list-28k-datapoints  BigBasket Entire Product List (~28K datapoints)      6MB  2022-06-22 12:51:18           1950         87  1.0              \n",
            "victorsoeiro/netflix-tv-shows-and-movies                  Netflix TV Shows and Movies                          2MB  2022-05-15 00:01:23          16605        483  1.0              \n",
            "sameepvani/nasa-nearest-earth-objects                     NASA - Nearest Earth Objects                         7MB  2022-06-17 02:32:18           2325        109  1.0              \n",
            "zusmani/petrolgas-prices-worldwide                        Petrol/Gas Prices Worldwide                         10KB  2022-06-24 01:25:33           2228        108  1.0              \n",
            "mukuldeshantri/ecommerce-fashion-dataset                  E-commerce Dataset with 30K Products               546KB  2022-07-08 12:28:18            586         26  1.0              \n",
            "jimschacko/airlines-dataset-to-predict-a-delay            Airlines Dataset to predict a delay                  6MB  2022-06-21 05:45:44           2073         61  1.0              \n",
            "iamsouravbanerjee/nifty500-stocks-dataset                 Stock Market Dataset (NIFTY-500)                    35KB  2022-07-17 11:46:10            662         54  1.0              \n",
            "zusmani/pakistans-national-assembly-attendance-20182022   Pakistan's National Assembly Attendance 2018-2022    2MB  2022-07-15 03:33:32            243         30  0.88235295       \n",
            "dansbecker/melbourne-housing-snapshot                     Melbourne Housing Snapshot                         451KB  2018-06-05 12:52:24          91127       1108  0.7058824        \n",
            "ramjasmaurya/1-gb-internet-price                          Internet Prices around 200+ countries in 2022.      22KB  2022-07-17 02:56:12           1115         39  0.9705882        \n",
            "datasnaek/youtube-new                                     Trending YouTube Video Statistics                  201MB  2019-06-03 00:56:47         179234       4599  0.7941176        \n",
            "zynicide/wine-reviews                                     Wine Reviews                                        51MB  2017-11-27 17:08:04         163077       3335  0.7941176        \n",
            "deepcontractor/car-price-prediction-challenge             Car Price Prediction Challenge                     429KB  2022-07-06 11:38:32            522         40  0.9411765        \n",
            "devansodariya/student-performance-data                    Student Performance Dataset                          7KB  2022-05-26 13:55:09           9710        253  0.9705882        \n",
            "datasnaek/chess                                           Chess Game Dataset (Lichess)                         3MB  2017-09-04 03:09:09          30468       1018  0.8235294        \n",
            "rtatman/188-million-us-wildfires                          1.88 Million US Wildfires                          168MB  2020-05-12 21:03:49          20704       1018  0.8235294        \n",
            "residentmario/ramen-ratings                               Ramen Ratings                                       40KB  2018-01-11 16:04:39          34819        792  0.7058824        \n",
            "dansbecker/powerlifting-database                          powerlifting-database                                9MB  2019-04-30 21:07:41           4919         59  0.5882353        \n",
            "jpmiller/publicassistance                                 US Public Food Assistance                          703KB  2020-08-21 16:51:18          16623        395  0.9117647        \n",
            "Downloading food11-image-dataset.zip to /content/sample_data\n",
            " 97% 1.06G/1.08G [00:09<00:00, 163MB/s]\n",
            "100% 1.08G/1.08G [00:09<00:00, 125MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from os import listdir\n",
        "onlyfiles = [f for f in listdir(\"/content/sample_data/training\")]\n",
        "print(onlyfiles)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-1Gv38qQT68",
        "outputId": "a406423c-522a-45db-8ef4-461331bbc192"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Dairy product', 'Bread', 'Seafood', 'Egg', 'Meat', 'Noodles-Pasta', 'Dessert', 'Vegetable-Fruit', 'Fried food', 'Rice', 'Soup']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-13T15:29:31.347090Z",
          "iopub.status.busy": "2021-03-13T15:29:31.346522Z",
          "iopub.status.idle": "2021-03-13T15:29:38.183618Z",
          "shell.execute_reply": "2021-03-13T15:29:38.184181Z"
        },
        "papermill": {
          "duration": 6.852121,
          "end_time": "2021-03-13T15:29:38.184377",
          "exception": false,
          "start_time": "2021-03-13T15:29:31.332256",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1vc97nUPGPk",
        "outputId": "68868e86-9434-4306-8d4b-d4b664af786c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 9866 files belonging to 11 classes.\n",
            "Found 3430 files belonging to 11 classes.\n",
            "Found 3347 files belonging to 11 classes.\n"
          ]
        }
      ],
      "source": [
        "# Load Data\n",
        "from os import listdir\n",
        "onlyfiles = [f for f in listdir(\"/content/sample_data/training\")]\n",
        "class_names = onlyfiles\n",
        "\n",
        "train_dir = \"/content/sample_data/training\"\n",
        "val_dir   = \"/content/sample_data/validation\"\n",
        "test_dir  = \"/content/sample_data/evaluation\"\n",
        "\n",
        "target_size = (224,224)\n",
        "batch_size = 16\n",
        "\n",
        "\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "                train_dir,\n",
        "                class_names = class_names,\n",
        "                batch_size = batch_size,\n",
        "             #   labels = 'inferred',\n",
        "                label_mode = 'categorical',\n",
        "                image_size = target_size,\n",
        "                shuffle = True,\n",
        ")\n",
        "\n",
        "valid_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "                val_dir,\n",
        "                class_names = class_names,\n",
        "                batch_size = batch_size,\n",
        "               # labels = 'inferred',\n",
        "                label_mode = 'categorical',\n",
        "                image_size = target_size,\n",
        "                shuffle = True,\n",
        ")\n",
        "\n",
        "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "                test_dir,\n",
        "                class_names = class_names,\n",
        "                batch_size = batch_size,\n",
        "              #  labels = 'inferred',\n",
        "                label_mode = 'categorical',\n",
        "                image_size = target_size,\n",
        "                shuffle = False,\n",
        ")\n",
        "\n",
        "# buffered prefetching\n",
        "train_ds = train_ds.prefetch(buffer_size=16)\n",
        "valid_ds = valid_ds.prefetch(buffer_size=16)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QF-Tsvcdv9q",
        "outputId": "e3bd023f-02f3-4236-cb15-83e2a00264eb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<BatchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 11), dtype=tf.float32, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-13T15:29:38.217255Z",
          "iopub.status.busy": "2021-03-13T15:29:38.216709Z",
          "iopub.status.idle": "2021-03-13T15:29:46.015034Z",
          "shell.execute_reply": "2021-03-13T15:29:46.015423Z"
        },
        "papermill": {
          "duration": 7.810504,
          "end_time": "2021-03-13T15:29:46.015575",
          "exception": false,
          "start_time": "2021-03-13T15:29:38.205071",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "py12oVuAPGPq",
        "outputId": "0534a759-6d5f-41f2-e684-2c6e174c3a1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "219062272/219055592 [==============================] - 2s 0us/step\n",
            "219070464/219055592 [==============================] - 2s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " rescaling (Rescaling)       (None, 224, 224, 3)       0         \n",
            "                                                                 \n",
            " inception_resnet_v2 (Functi  (None, 5, 5, 1536)       54336736  \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 1536)             0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " dense (Dense)               (None, 11)                16907     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 54,353,643\n",
            "Trainable params: 54,293,099\n",
            "Non-trainable params: 60,544\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# Definir le modèle\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.experimental.preprocessing.Rescaling(1./255, input_shape=(224,224, 3)))\n",
        "model.add(tf.keras.applications.InceptionResNetV2(input_shape=(224,224, 3), include_top=False))\n",
        "model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
        "model.add(tf.keras.layers.Dense(11, activation='softmax'))\n",
        "\n",
        "print(model.summary()) \n",
        "#tf.keras.utils.plot_model(model, show_shapes=True) # graph du modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-13T15:29:46.073106Z",
          "iopub.status.busy": "2021-03-13T15:29:46.070484Z",
          "iopub.status.idle": "2021-03-13T15:51:09.108042Z",
          "shell.execute_reply": "2021-03-13T15:51:09.108728Z"
        },
        "papermill": {
          "duration": 1283.077558,
          "end_time": "2021-03-13T15:51:09.108910",
          "exception": false,
          "start_time": "2021-03-13T15:29:46.031352",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7axfcSEPGPs",
        "outputId": "faf69fb5-b71a-4af1-9f30-c8730b5ddb47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "617/617 [==============================] - 261s 372ms/step - loss: 1.0367 - accuracy: 0.6544 - auc: 0.9433 - val_loss: 1.7226 - val_accuracy: 0.5822 - val_auc: 0.9032\n",
            "Epoch 2/15\n",
            "617/617 [==============================] - 228s 369ms/step - loss: 0.6606 - accuracy: 0.7863 - auc: 0.9751 - val_loss: 1.0354 - val_accuracy: 0.6936 - val_auc: 0.9461\n",
            "Epoch 3/15\n",
            "617/617 [==============================] - 228s 369ms/step - loss: 0.5082 - accuracy: 0.8277 - auc: 0.9848 - val_loss: 1.0927 - val_accuracy: 0.7009 - val_auc: 0.9439\n",
            "Epoch 4/15\n",
            "617/617 [==============================] - 228s 370ms/step - loss: 0.3859 - accuracy: 0.8723 - auc: 0.9900 - val_loss: 2.1541 - val_accuracy: 0.5673 - val_auc: 0.8769\n",
            "Epoch 5/15\n",
            "487/617 [======================>.......] - ETA: 43s - loss: 0.3204 - accuracy: 0.8916 - auc: 0.9929"
          ]
        }
      ],
      "source": [
        "# Train\n",
        "\n",
        "model.compile(\n",
        "                  loss = \"categorical_crossentropy\",\n",
        "              optimizer='adam',\n",
        "                  metrics = ['accuracy', 'AUC']) # AUC = Area under the ROC Curve\n",
        "    \n",
        "history = model.fit(train_ds, epochs = 15, validation_data = valid_ds) # Train and validate\n",
        "\n",
        "#Je dois add 20 epochs, loss still too high"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-13T15:51:09.945867Z",
          "iopub.status.busy": "2021-03-13T15:51:09.945008Z",
          "iopub.status.idle": "2021-03-13T15:52:01.535227Z",
          "shell.execute_reply": "2021-03-13T15:52:01.534384Z"
        },
        "papermill": {
          "duration": 52.006785,
          "end_time": "2021-03-13T15:52:01.535351",
          "exception": false,
          "start_time": "2021-03-13T15:51:09.528566",
          "status": "completed"
        },
        "tags": [],
        "id": "aMKJ1bDkPGPu"
      },
      "outputs": [],
      "source": [
        "# Test\n",
        "model.evaluate(test_ds)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(labels)"
      ],
      "metadata": {
        "id": "pTQoRDR5bUrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=target_size,\n",
        "    batch_size=batch_size,\n",
        "    color_mode='rgb',\n",
        "    shuffle=False,    \n",
        "    class_mode='categorical')"
      ],
      "metadata": {
        "id": "o6mQatfldlk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-13T15:52:02.409178Z",
          "iopub.status.busy": "2021-03-13T15:52:02.408404Z",
          "iopub.status.idle": "2021-03-13T15:52:44.400146Z",
          "shell.execute_reply": "2021-03-13T15:52:44.399693Z"
        },
        "papermill": {
          "duration": 42.434695,
          "end_time": "2021-03-13T15:52:44.400273",
          "exception": false,
          "start_time": "2021-03-13T15:52:01.965578",
          "status": "completed"
        },
        "tags": [],
        "id": "azVIS81XPGPx"
      },
      "outputs": [],
      "source": [
        "#Confusion Matrix\n",
        "#preds=model.predict(test_ds)\n",
        "predsTwo=model.predict(test_generator)\n",
        "y_pred = np.argmax(predsTwo,axis=1)\n",
        "\n",
        "y_true =[]\n",
        "for images, labels in test_ds.unbatch().take(3347):\n",
        "  y_true.append(labels.numpy())\n",
        "y_true = np.array(y_true)\n",
        "\n",
        "y_true = np.argmax(y_true)\n",
        "\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print('Confusion Matrix\\n', cm)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Confusion Matrix\n",
        "\n",
        "y_pred = model.predict(test_ds, batch_size=32)\n",
        "    \n",
        "y_true = []\n",
        "for images, labels in test_ds.unbatch().take(3347):\n",
        "    y_true.append(labels.numpy())\n",
        "y_true = np.array(y_true)\n",
        "\n",
        "# Convert one-hot encoding to integers\n",
        "y_true = np.argmax(y_true, axis=1)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred) # Generate Confusion Matrix\n",
        "print(cm)\n",
        "#df_cm = pd.DataFrame(cm, index = [i for i in class_names], columns = [i for i in class_names])\n",
        "#matplotlib.pyplot.figure(figsize = (10,8))\n",
        "#sn.heatmap(df_cm, annot=True)\n",
        "#matplotlib.pyplot.savefig('./confusion_matrix')\n",
        "'''\n",
        "class_dist = []\n",
        "for i in range(11):\n",
        "    class_dist.append( np.sum(y_true == i))\n",
        "print(\"True distribution of classes are:\")\n",
        "\n",
        "print(np.around(class_dist / np.sum(class_dist) * 100, 2)) # True distribution of classes\n",
        "'''\n"
      ],
      "metadata": {
        "id": "mdW02H1ufoSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "pip install scikit-plot\n",
        "import scikitplot as skplt\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve\n",
        "\n",
        "preds=model.predict(test_generator)\n",
        "y_pred = np.argmax(preds,axis=1)\n",
        "\n",
        "y_true = test_generator.classes\n",
        "print(classification_report(y_true, y_pred, target_names=labels))\n",
        "\n",
        "\n",
        "\n",
        "#\n",
        "roc = roc_curve(y_true, y_pred)\n",
        "skplt.metrics.plot_roc(y_true, y_pred, pos_label=1)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#\n",
        "scikitplot.metrics.plot_lift_curve(y_true, y_pred, title='Lift Curve')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Sh0uzatKRuFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Je peux essayer aussi \n",
        "'''\n",
        "pip install scikit-plot\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, auc, roc_curve\n",
        "preds = model.predict(test_generator)\n",
        "\n",
        "y_pred = np.argmax(preds,axis=1)\n",
        "y_true = test_generator.classes\n",
        "fpr, tpr,  _  = roc_curve(y_true, y_pred, pos_label=1)\n",
        "#roc_auc_score(y_true, y_pred)\n",
        "#print (auc(fprate, tprate))\n",
        "#print ('ROC Score:', roc_auc_score(y_true, y_pred, pos_label=1))\n",
        "plt.plot(fpr,tpr)\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "#or \n",
        "'''\n",
        "import scikitplot as skplt\n",
        "from sklearn.metrics import roc_curve\n",
        "from matplotlib import pyplot as plt\n",
        "preds = model.predict(test_generator)\n",
        "\n",
        "y_pred = np.argmax(preds,axis=1)\n",
        "y_true = test_generator.classes\n",
        "skplt.metrics.plot_roc(y_true, y_pred, pos_label=1)\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "#or\n",
        "\n",
        "'''\n",
        "from sklearn.metrics import roc_curve\n",
        "preds=model.predict(test_generator)\n",
        "y_pred = np.argmax(preds,axis=1)\n",
        "y_true = test_generator.classes\n",
        "roc = roc_curve(y_true, y_pred)\n",
        "'''"
      ],
      "metadata": {
        "id": "KGXfo0p0TCHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = keras.preprocessing.image.load_img(\n",
        "    \"/content/sample_data/training/Fried food/209.jpg\", target_size=(224, 224)\n",
        ")\n",
        "img_array = keras.preprocessing.image.img_to_array(img)\n",
        "img_array = tf.expand_dims(img_array, 0)  # Create batch axis\n",
        "\n",
        "predictions = model.predict(img_array)\n",
        "score = predictions[0][0]\n",
        "print(\n",
        "    \"This image is %.2f percent dessert and %.2f percent not dessert.\"\n",
        "    % (100 * (1 - score), 100 * score)\n",
        ")\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "8GvGYuaOSJSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-13T15:52:45.309664Z",
          "iopub.status.busy": "2021-03-13T15:52:45.308575Z",
          "iopub.status.idle": "2021-03-13T15:52:46.400410Z",
          "shell.execute_reply": "2021-03-13T15:52:46.400808Z"
        },
        "papermill": {
          "duration": 1.551335,
          "end_time": "2021-03-13T15:52:46.400959",
          "exception": false,
          "start_time": "2021-03-13T15:52:44.849624",
          "status": "completed"
        },
        "tags": [],
        "id": "7RWh3WFYPGPz"
      },
      "outputs": [],
      "source": [
        "# Metrics plots\n",
        "matplotlib.pyplot.figure()\n",
        "matplotlib.pyplot.plot(history.history['loss'])\n",
        "matplotlib.pyplot.savefig('./loss')\n",
        "matplotlib.pyplot.figure()\n",
        "matplotlib.pyplot.plot(history.history['accuracy'])\n",
        "matplotlib.pyplot.savefig('./accuracy')\n",
        "matplotlib.pyplot.figure()\n",
        "matplotlib.pyplot.plot(history.history['auc'])\n",
        "matplotlib.pyplot.savefig('./auc')\n",
        "matplotlib.pyplot.figure()\n",
        "matplotlib.pyplot.plot(history.history['val_loss'])\n",
        "matplotlib.pyplot.savefig('./val_loss')\n",
        "matplotlib.pyplot.figure()\n",
        "matplotlib.pyplot.plot(history.history['val_accuracy'])\n",
        "matplotlib.pyplot.savefig('./val_accuracy')\n",
        "matplotlib.pyplot.figure()\n",
        "matplotlib.pyplot.plot(history.history['val_auc'])\n",
        "matplotlib.pyplot.savefig('./val_auc')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-13T15:52:47.400090Z",
          "iopub.status.busy": "2021-03-13T15:52:47.398156Z",
          "iopub.status.idle": "2021-03-13T15:55:46.108933Z",
          "shell.execute_reply": "2021-03-13T15:55:46.107861Z"
        },
        "papermill": {
          "duration": 179.27218,
          "end_time": "2021-03-13T15:55:46.109089",
          "exception": false,
          "start_time": "2021-03-13T15:52:46.836909",
          "status": "completed"
        },
        "tags": [],
        "id": "oZYbX9WpPGP1"
      },
      "outputs": [],
      "source": [
        "# Save model and weights\n",
        "model.save('./model')\n",
        "model.save_weights('./weights')\n",
        "!zip -r -q Adamax4_plots.zip ./confusion_matrix.png ./loss.png ./accuracy ./auc.png ./val_loss.png ./val_accuracy.png ./val_auc.png # Zip all plots\n",
        "!zip -r -q Adamax4.zip ./model ./confusion_matrix.png ./weights.data-00000-of-00001 ./weights.index ./loss ./accuracy ./auc ./val_loss ./val_accuracy ./val_auc"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 1592.252192,
      "end_time": "2021-03-13T15:55:52.878191",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-03-13T15:29:20.625999",
      "version": "2.2.2"
    },
    "colab": {
      "name": "food11-classifier.ipynb",
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}